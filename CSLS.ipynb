{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b4740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.linalg import svd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "from sklearn.preprocessing import normalize\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae09d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3547b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5993f28",
   "metadata": {},
   "source": [
    "The pre-trained fasttext model for english and hindi langauges are obtained from \n",
    "https://fasttext.cc/docs/en/pretrained-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "893c9702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained FastText models for English and Hindi\n",
    "#model_hi = fasttext.load_model(\"wiki_hindi/wiki.hi.bin\")  # Path to English model bin\n",
    "#model_en = fasttext.load_model(\"wiki_english/wiki.en.bin\")  # Path to Hindi model bin\n",
    "\n",
    "en_embeddings = KeyedVectors.load_word2vec_format('cc.en.300.vec.gz', limit=200000)\n",
    "hi_embeddings = KeyedVectors.load_word2vec_format('cc.hi.300.vec.gz', limit=200000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "110a0454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 200000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the words in vocab\n",
    "vocab_en = []\n",
    "for each in en_embeddings.key_to_index:\n",
    "    vocab_en.append(each)\n",
    "vocab_hi = []   \n",
    "for each in hi_embeddings.key_to_index:\n",
    "    vocab_hi.append(each)\n",
    "len(vocab_en), len(vocab_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5577ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_vectors = en_embeddings.vectors\n",
    "hi_embeddings_vectors = hi_embeddings.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c32e5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_matrix = np.load('unsupervised_w_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19678a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english embeddings shape (22733, 300) hindi embeddings shape (22733, 300)\n"
     ]
    }
   ],
   "source": [
    "# read the file where english and hindi words present\n",
    "# separate hindi and english words\n",
    "# get embeddings for the words also from respective models.\n",
    "file_name = 'en-hi.txt'\n",
    "\n",
    "eng_hin_lexicon = []\n",
    "english_embs_lexicon = []\n",
    "hindi_emds_lexicon = []\n",
    "\n",
    "with open(file_name, 'r') as file:\n",
    "    for line in file:\n",
    "        words = line.split('\\t')\n",
    "        eng = words[0]\n",
    "        hin = words[1].split('\\n')[0]\n",
    "        if eng in vocab_en and hin in vocab_hi:\n",
    "            eng_hin_lexicon.append((eng, hin))\n",
    "            hin_vector = hi_embeddings_vectors[vocab_hi.index(hin)]\n",
    "            eng_vector = en_embeddings_vectors[vocab_en.index(eng)]\n",
    "            english_embs_lexicon.append(eng_vector)\n",
    "            hindi_emds_lexicon.append(hin_vector)\n",
    "english_embs_lexicon = np.array(english_embs_lexicon)\n",
    "hindi_emds_lexicon = np.array(hindi_emds_lexicon)\n",
    "print('english embeddings shape', english_embs_lexicon.shape, 'hindi embeddings shape', hindi_emds_lexicon.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c4eef",
   "metadata": {},
   "source": [
    "# using W_matrix instead of R (cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cbfe964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train set precision values\n",
      "precision@1 0.0\n",
      "precision@5 0.0\n"
     ]
    }
   ],
   "source": [
    "# Precision@1 and Precision@5 accuracy\n",
    "def precision(word_set, R, hindi_vocab_embeddings, vocab_hi, en_embeddings, vocab_en, k= 5):\n",
    "    p_1 = 0\n",
    "    p_5 = 0\n",
    "    for i, (eng, hin) in enumerate(word_set):\n",
    "        word_vector = en_embeddings[vocab_en.index(eng)]\n",
    "        word_vector = en_embeddings[vocab_en.index(eng)]\n",
    "        aligned_vector = np.dot(word_vector, R)\n",
    "        aligned_vector = aligned_vector.reshape(1, -1)\n",
    "        similarities = cosine_similarity(aligned_vector, hindi_vocab_embeddings)[0]\n",
    "        most_similar_idx = similarities.argsort()[-k:][::-1]\n",
    "        sims = [vocab_hi[top] for top in most_similar_idx]\n",
    "        \n",
    "        if sims[0] == hin:\n",
    "            p_1 += 1\n",
    "        if hin in sims:\n",
    "            p_5 += 1\n",
    "    print('precision@1' ,p_1 / len(word_set))\n",
    "    print('precision@5', p_5 / len(word_set))\n",
    "    \n",
    "print('The train set precision values') \n",
    "precision(eng_hin_lexicon[:100], W_matrix, hi_embeddings_vectors, vocab_hi, en_embeddings_vectors, vocab_en, k = 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45dadd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train set precision values\n",
      "English: and Hindi: और Predicted hindi: जेहनी with cosine similarity: 0.23745158\n",
      "English: was Hindi: था Predicted hindi: जेहनी with cosine similarity: 0.23412967\n",
      "English: was Hindi: थी Predicted hindi: जेहनी with cosine similarity: 0.23412967\n",
      "English: for Hindi: लिये Predicted hindi: जेहनी with cosine similarity: 0.24661002\n",
      "English: that Hindi: उस Predicted hindi: जेहनी with cosine similarity: 0.2455211\n",
      "English: that Hindi: कि Predicted hindi: जेहनी with cosine similarity: 0.2455211\n",
      "English: with Hindi: साथ Predicted hindi: जेहनी with cosine similarity: 0.24153118\n",
      "English: from Hindi: से Predicted hindi: जेहनी with cosine similarity: 0.22953205\n",
      "English: from Hindi: इससे Predicted hindi: जेहनी with cosine similarity: 0.22953205\n",
      "English: this Hindi: ये Predicted hindi: जेहनी with cosine similarity: 0.23956154\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def translate_word(en_embeddings, word_set, R, hi_embeddings, vocab_hi, vocab_en):\n",
    "    \n",
    "    for i, (eng, hin) in enumerate(word_set):\n",
    "        word_vector = en_embeddings[vocab_en.index(eng)]\n",
    "        aligned_vector = np.dot(word_vector, R)\n",
    "        aligned_vector = aligned_vector.reshape(1, -1)\n",
    "        similarities = cosine_similarity(aligned_vector, hi_embeddings.vectors)[0]\n",
    "        similarity_of_first = max(similarities)\n",
    "        most_similar_idx = similarities.argmax()\n",
    "        sims = vocab_hi[most_similar_idx] \n",
    "        print('English:',eng ,'Hindi:', hin, 'Predicted hindi:',sims, 'with cosine similarity:', similarity_of_first)\n",
    "        \n",
    "print('The train set precision values') \n",
    "translate_word(en_embeddings, eng_hin_lexicon[:10], W_matrix, hi_embeddings, vocab_hi, vocab_en)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6559fab0",
   "metadata": {},
   "source": [
    "# CSLS\n",
    "In high-dimensional spaces (like 300D word embeddings), some vectors (words) tend to be \"close\" to many other vectors, even though they aren’t really semantically similar. These vectors are called hubs.\n",
    "- \"apple\" → \"सेब\"\n",
    "- \"apple\" → \"है\" or \"की\" or \"यह\"\n",
    "\n",
    "Because these common Hindi words (\"है\", \"की\", etc.) are very similar to everything in the space — they’re hubs.\n",
    "\n",
    "Hubness breaks nearest neighbor search. Instead of finding the real translation, model often picks a hub word that’s “close” to too many things, just because it's sitting in the center of the embedding space.\n",
    "\n",
    "Regular cosine similarity only checks how close x and y are — ignoring how many other words y is close to.\n",
    "\n",
    "CSLS = 2cos(x,y) - r_x - r_y\n",
    "- CSLS takes into account: \n",
    "    - r_x = avg similarity of x to its k nearest neighbors in the target space.\n",
    "    - r_y = avg similarity of y to its k nearest neighbors in the source space.\n",
    "    - If y is a hub, r_y will be high, and CSLS(x, y) will decrease → penalized.\n",
    "    - If y is really a unique match for x, r_y will be low, so CSLS will keep a high score.\n",
    "- example x = \"apple\" and y = \"है\" (very common Hindi word, a hub)\n",
    "    - cos(x, y) might be high (because \"है\" is close to everything).\n",
    "    - r_y (how close y is to many other source words) will also be very high. Because \"है\" is close to all English words (it's a hub).\n",
    "    - The result? CSLS drops → \"है\" is penalized.\n",
    "- y = \"सेब\" (actual Hindi translation)\n",
    "    - cos(x, y) is high. \n",
    "    - r_y is low — \"सेब\" is not a hub, it’s close only to relevant English words (like \"apple\", \"fruit\").\n",
    "    - CSLS remains high → \"सेब\" is rewarded.\n",
    "    \n",
    "So CSLS likes vectors that are uniquely close to the source word, not vectors that are close to everything. It adjusts for local density of the embedding space. Hubs are penalized because they have high average similarity with everyone. True matches stand out because they have meaningful, localized similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d7c15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_average_knn_sim(emb, other_embs, k=10):\n",
    "    sim = cosine_similarity(emb.reshape(1, -1), other_embs)[0]\n",
    "    top_k = np.sort(sim)[-k:]\n",
    "    return np.mean(top_k)\n",
    "\n",
    "def compute_csls_scores(src_vec, tgt_matrix, avg_sim_tgt, avg_sim_src_vecs):\n",
    "    sim = cosine_similarity(src_vec.reshape(1, -1), tgt_matrix)[0]\n",
    "    csls = 2 * sim - avg_sim_tgt - avg_sim_src_vecs\n",
    "    return csls\n",
    "def translate_word_csls(src_vec, tgt_matrix, tgt_vocab, src_matrix, k=10):\n",
    "    avg_sim_tgt = np.array([\n",
    "        get_average_knn_sim(tgt_vec, src_matrix, k) for tgt_vec in tgt_matrix\n",
    "    ])\n",
    "    avg_sim_src = get_average_knn_sim(src_vec, tgt_matrix, k)\n",
    "    csls_scores = compute_csls_scores(src_vec, tgt_matrix, avg_sim_tgt, avg_sim_src)\n",
    "\n",
    "    top_k_indices = csls_scores.argsort()[-k:][::-1]\n",
    "    return [tgt_vocab[i] for i in top_k_indices]\n",
    "\n",
    "src_word = 'apple'\n",
    "src_vec = np.dot(en_embeddings_vectors[vocab_en.index(src_word)], W_matrix.T)\n",
    "top_translations = translate_word_csls(src_vec, hi_embeddings_vectors, vocab_hi, en_embeddings_vectors)\n",
    "print(\"Top Hindi translations:\", top_translations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b086fd",
   "metadata": {},
   "source": [
    "# CSLS implementation with faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d89ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_average_knn_sim(emb, other_embs, k=10):\n",
    "    faiss_index = faiss.IndexFlatIP(300)\n",
    "    faiss_index.add(np.array(other_embs, dtype=np.float32))\n",
    "    distances, indices = faiss_index.search(np.array([emb], dtype=np.float32), k)\n",
    "    distances = distances.squeeze(0)    \n",
    "    top_k = np.sort(distances)[-k:]\n",
    "    return np.mean(top_k)\n",
    "\n",
    "def compute_csls_scores(src_vec, tgt_matrix, avg_sim_tgt, avg_sim_src_vecs):\n",
    "    sim = cosine_similarity(src_vec.reshape(1, -1), tgt_matrix)[0]\n",
    "    csls = 2 * sim - avg_sim_tgt - avg_sim_src_vecs\n",
    "    return csls\n",
    "\n",
    "def translate_word_csls(src_vec, tgt_matrix, tgt_vocab, src_matrix, k=10):\n",
    "    avg_sim_tgt = np.array([get_average_knn_sim(tgt_vec, src_matrix, k) for tgt_vec in tgt_matrix])\n",
    "    avg_sim_src = get_average_knn_sim(src_vec, tgt_matrix, k)\n",
    "    csls_scores = compute_csls_scores(src_vec, tgt_matrix, avg_sim_tgt, avg_sim_src)\n",
    "    top_k_indices = csls_scores.argsort()[-k:][::-1]\n",
    "    return [tgt_vocab[i] for i in top_k_indices]\n",
    "\n",
    "src_word = 'apple'\n",
    "src_vec = np.dot(en_embeddings_vectors[vocab_en.index(src_word)], W_matrix.T)\n",
    "top_translations = translate_word_csls(src_vec, hi_embeddings_vectors, vocab_hi, en_embeddings_vectors)\n",
    "print(\"Top Hindi translations:\", top_translations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59696306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a37620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4d37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395c3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
